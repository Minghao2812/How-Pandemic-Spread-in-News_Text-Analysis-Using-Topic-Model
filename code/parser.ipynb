{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import bs4 as bs  # import BeautifulSoup as bs\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import date\n",
    "import os\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "\n",
    "from six.moves.urllib import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_parser(news_url):\n",
    "    \"\"\"parse news from HTML to a readable article\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Parsing news at\\n %s\" % news_url)\n",
    "    news_id = re.search(pattern=\"\\d{7}\", string=news_url).group(0)\n",
    "    news_title = (news_url.split(\"/\"))[-1]\n",
    "\n",
    "    news_page = requests.get(news_url)\n",
    "    news_page.encoding = \"utf-8\"\n",
    "    news = bs.BeautifulSoup(news_page.text, \"lxml\")\n",
    "\n",
    "    article = [\"\"]  # pseudo initialization, in the case some news cannot be parsed\n",
    "\n",
    "    # step1: extract all HTML strings containing 'APOLLO_STATE'\n",
    "    APOLLO_string = []\n",
    "    scripts = news.find_all(\"script\")\n",
    "    for script in scripts:\n",
    "        sString = script.string\n",
    "        if sString != None and sString.find(\"APOLLO_STATE\") > 0:\n",
    "            APOLLO_string.append(sString)\n",
    "\n",
    "    # step2: extract contents from {}\n",
    "    if APOLLO_string != []:\n",
    "        regBrackets = r\"\\{(.*?)\\}\"\n",
    "        matches = re.finditer(regBrackets, APOLLO_string[0], re.MULTILINE | re.DOTALL)\n",
    "\n",
    "        bracket_content = []\n",
    "        # bracket_content is the content between {}\n",
    "        for match in matches:\n",
    "            bracket_content.append(match.group(1))\n",
    "\n",
    "        # step3: match sentence\n",
    "        article_end_index = bracket_content.index(\n",
    "            '\"type\":\"reading-100-percent-completion-tracker\"'\n",
    "        )\n",
    "        sentence = []\n",
    "        # sentence[] contains the sentence behind \"type\":\"text\",\"data\":, and that's the article sentence\n",
    "        sen_code = '(?<=\"type\":\"p\",\"children\":\\[{\"type\":\"text\",\"data\":).*$'\n",
    "        for content in bracket_content[0 : article_end_index + 1]:\n",
    "            sen = re.findall(sen_code, content)\n",
    "            if not sen == [] and not sen == ['\"\\\\n\"']:  # clean blank lists and \\\\n\n",
    "                sentence += sen\n",
    "\n",
    "        # step4: clean sentences and combine them into an article\n",
    "        for i in range(len(sentence)):\n",
    "            sentence[i] = sentence[i].strip('\"')\n",
    "        article = \"\"\n",
    "\n",
    "        for s in sentence:\n",
    "            article += s\n",
    "            # change: new sentence has a space ahead\n",
    "            article += \" \"\n",
    "        article = [article]\n",
    "\n",
    "    # record date and build a dataframe\n",
    "    news_time = news.find(\"time\")\n",
    "    if news_time == None:\n",
    "        news_time = [\"cannot parse\"]\n",
    "    else:\n",
    "        news_time = news_time[\"datetime\"]\n",
    "    now = date.today()\n",
    "\n",
    "    news_df = pd.DataFrame(\n",
    "        data={\n",
    "            \"news_id\": news_id,\n",
    "            \"title\": news_title,\n",
    "            \"article\": article,\n",
    "            \"release_time\": news_time,\n",
    "            \"collecting_date\": now,\n",
    "            \"URL\": news_url,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    time.sleep(random.random() * 5)\n",
    "    return news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_parser(comments_url):\n",
    "    \"\"\"parse comments and return a dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Parsing comments at\\n %s\" % comments_url)\n",
    "    comments_page = requests.get(comments_url)\n",
    "    comments_page.encoding = \"utf-8\"\n",
    "    comments = bs.BeautifulSoup(comments_page.text, \"lxml\")\n",
    "\n",
    "    words = comments.find_all(class_=\"card-content-action\")\n",
    "    # the very raw comments, every \"card-content-action\" is a person's comment block.\n",
    "    user_name = []\n",
    "    sentence = []\n",
    "    comment_time = []\n",
    "    for word in words:  # every word is a comment block\n",
    "        user_name.append(word.find(attrs={\"class\": \"comment-author-name\"}).text)\n",
    "        sentence.append(\n",
    "            word.find(attrs={\"class\": \"comment-content\"})\n",
    "            .text.strip()\n",
    "            .replace(\"\\n\", \" \")\n",
    "        )\n",
    "        comment_time.append((word.find(\"time\"))[\"datetime\"])\n",
    "\n",
    "    sentence_cleaned = []\n",
    "    for s in sentence:\n",
    "        if \"@******\" in s:\n",
    "            sentence_cleaned.append((s.split(\"@******\"))[1].strip())\n",
    "        else:\n",
    "            sentence_cleaned.append(s)\n",
    "\n",
    "    news_id = comments_url[-7:]\n",
    "    now = date.today()\n",
    "    comments_df = pd.DataFrame(\n",
    "        data={\n",
    "            \"user_name\": user_name,\n",
    "            \"comment_raw\": sentence,\n",
    "            \"comment_cleaned\": sentence_cleaned,\n",
    "            \"date\": comment_time,\n",
    "            \"news_id\": news_id,\n",
    "            \"is_reply\": True,\n",
    "            \"collecting_date\": now,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for index, row in comments_df.iterrows():\n",
    "        if \"@******\" in row[\"comment_raw\"]:\n",
    "            comments_df.at[index, \"is_reply\"] = True\n",
    "        else:\n",
    "            comments_df.at[index, \"is_reply\"] = False\n",
    "\n",
    "    time.sleep(random.random() * 3)\n",
    "    return comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all URLs of the articles\n",
    "**Method**  \n",
    "Since SCMP uses infinite roll to load old articles, I catch the URL of every request and find its pattern. Some short pages use 0,20,40,60 as their identifier, while other pages may use timestamps. For those timestamps, I collect them manually.\n",
    "I've also tried to download rolled down pages as mHTML, but bs.find_all(\"a\", href=True) or bs.find_all(class_=) doesn't work as in online pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scmp.com/coronavirus/greater-china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    url = \"https://apigw.scmp.com/content-delivery/v1?operationName=QueueById&variables=%7B%22itemLimit%22%3A20%2C%22offset%22%3A{offset}%2C%22name%22%3A%22section_top_505356%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%223f30ddb476061fe29f0dba291c35330b61a58fb0aaf08096d985f6b81e107840%22%7D%7D\".format(\n",
    "        offset=i * 20\n",
    "    )\n",
    "    headers = {\"apikey\": \"MyYvyg8M9RTaevVlcIRhN5yRIqqVssNY\"}\n",
    "    text = requests.get(url=url, headers=headers).text\n",
    "    result = json.loads(text)[\"data\"][\"queue\"][\"items\"]\n",
    "    for item in result:\n",
    "        url_list.append(item[\"urlAlias\"])\n",
    "        # print(item[\"urlAlias\"])  # item[\"headline\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scmp.com/coronavirus/asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-60,step=20\n",
    "for i in range(4):\n",
    "    url = \"https://apigw.scmp.com/content-delivery/v1?operationName=QueueById&variables=%7B%22itemLimit%22%3A20%2C%22offset%22%3A{offset}%2C%22name%22%3A%22section_top_505326%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%223f30ddb476061fe29f0dba291c35330b61a58fb0aaf08096d985f6b81e107840%22%7D%7D\".format(\n",
    "        offset=i * 20\n",
    "    )\n",
    "    headers = {\"apikey\": \"MyYvyg8M9RTaevVlcIRhN5yRIqqVssNY\"}\n",
    "    text = requests.get(url=url, headers=headers).text\n",
    "    result = json.loads(text)[\"data\"][\"queue\"][\"items\"]\n",
    "    for item in result:\n",
    "        url_list.append(item[\"urlAlias\"])\n",
    "        # print(item[\"urlAlias\"])  # item[\"headline\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scmp.com/coronavirus/europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-60,step=20\n",
    "for i in range(4):\n",
    "    url = \"https://apigw.scmp.com/content-delivery/v1?operationName=QueueById&variables=%7B%22itemLimit%22%3A20%2C%22offset%22%3A{offset}%2C%22name%22%3A%22section_top_505325%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%223f30ddb476061fe29f0dba291c35330b61a58fb0aaf08096d985f6b81e107840%22%7D%7D\".format(\n",
    "        offset=i * 20\n",
    "    )\n",
    "    headers = {\"apikey\": \"MyYvyg8M9RTaevVlcIRhN5yRIqqVssNY\"}\n",
    "    text = requests.get(url=url, headers=headers).text\n",
    "    result = json.loads(text)[\"data\"][\"queue\"][\"items\"]\n",
    "    for item in result:\n",
    "        url_list.append(item[\"urlAlias\"])\n",
    "        # print(item[\"urlAlias\"])  # item[\"headline\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scmp.com/coronavirus/us-canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-60,step=20\n",
    "for i in range(4):\n",
    "    url = \"https://apigw.scmp.com/content-delivery/v1?operationName=QueueById&variables=%7B%22itemLimit%22%3A20%2C%22offset%22%3A{offset}%2C%22name%22%3A%22section_top_505354%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%223f30ddb476061fe29f0dba291c35330b61a58fb0aaf08096d985f6b81e107840%22%7D%7D\".format(\n",
    "        offset=i * 20\n",
    "    )\n",
    "    headers = {\"apikey\": \"MyYvyg8M9RTaevVlcIRhN5yRIqqVssNY\"}\n",
    "    text = requests.get(url=url, headers=headers).text\n",
    "    result = json.loads(text)[\"data\"][\"queue\"][\"items\"]\n",
    "    for item in result:\n",
    "        url_list.append(item[\"urlAlias\"])\n",
    "        # print(item[\"urlAlias\"])  # item[\"headline\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scmp.com/coronavirus/health-medicine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-60,step=20\n",
    "for i in range(4):\n",
    "    url = \"https://apigw.scmp.com/content-delivery/v1?operationName=QueueById&variables=%7B%22itemLimit%22%3A20%2C%22offset%22%3A{offset}%2C%22name%22%3A%22section_top_505355%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%223f30ddb476061fe29f0dba291c35330b61a58fb0aaf08096d985f6b81e107840%22%7D%7D\".format(\n",
    "        offset=i * 20\n",
    "    )\n",
    "    headers = {\"apikey\": \"MyYvyg8M9RTaevVlcIRhN5yRIqqVssNY\"}\n",
    "    text = requests.get(url=url, headers=headers).text\n",
    "    result = json.loads(text)[\"data\"][\"queue\"][\"items\"]\n",
    "    for item in result:\n",
    "        url_list.append(item[\"urlAlias\"])\n",
    "        # print(item[\"urlAlias\"])  # item[\"headline\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scmp.com/coronavirus/economic-impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-60,step=20\n",
    "for i in range(4):\n",
    "    url = \"https://apigw.scmp.com/content-delivery/v1?operationName=QueueById&variables=%7B%22itemLimit%22%3A20%2C%22offset%22%3A{offset}%2C%22name%22%3A%22section_top_505328%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%223f30ddb476061fe29f0dba291c35330b61a58fb0aaf08096d985f6b81e107840%22%7D%7D\".format(\n",
    "        offset=i * 20\n",
    "    )\n",
    "    headers = {\"apikey\": \"MyYvyg8M9RTaevVlcIRhN5yRIqqVssNY\"}\n",
    "    text = requests.get(url=url, headers=headers).text\n",
    "    result = json.loads(text)[\"data\"][\"queue\"][\"items\"]\n",
    "    for item in result:\n",
    "        url_list.append(item[\"urlAlias\"])\n",
    "        # print(item[\"urlAlias\"])  # item[\"headline\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scmp.com/coronavirus/analysis-opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-60,step=20\n",
    "for i in range(4):\n",
    "    url = \"https://apigw.scmp.com/content-delivery/v1?operationName=QueueById&variables=%7B%22itemLimit%22%3A20%2C%22offset%22%3A{offset}%2C%22name%22%3A%22section_top_505327%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%223f30ddb476061fe29f0dba291c35330b61a58fb0aaf08096d985f6b81e107840%22%7D%7D\".format(\n",
    "        offset=i * 20\n",
    "    )\n",
    "    headers = {\"apikey\": \"MyYvyg8M9RTaevVlcIRhN5yRIqqVssNY\"}\n",
    "    text = requests.get(url=url, headers=headers).text\n",
    "    result = json.loads(text)[\"data\"][\"queue\"][\"items\"]\n",
    "    for item in result:\n",
    "        url_list.append(item[\"urlAlias\"])\n",
    "        # print(item[\"urlAlias\"])  # item[\"headline\"],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.scmp.com/topics/coronavirus-china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1579925153000',\n",
       " '1581608419000',\n",
       " '1584493217000',\n",
       " '1584581236000',\n",
       " '1584768613000',\n",
       " '1584874812000',\n",
       " '1585132217000',\n",
       " '1585229419000',\n",
       " '1585400413000',\n",
       " '1585528208000',\n",
       " '1585735387000',\n",
       " '1585896342000',\n",
       " '1585954821000',\n",
       " '1586317395000',\n",
       " '1586361914000',\n",
       " '1586527218000',\n",
       " '1586610083000',\n",
       " '1586842439000',\n",
       " '1586913303000',\n",
       " '1587036700000',\n",
       " '1587117610000',\n",
       " '1587295804000',\n",
       " '1587436199000',\n",
       " '1587558821000',\n",
       " '1587770106000',\n",
       " '1588063310000',\n",
       " '1588345217000',\n",
       " '1588673798000',\n",
       " '1588939218000',\n",
       " '1589274011000',\n",
       " '1589540408000']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_list_COVCHINA = [\n",
    "    \"1587436199000\",\n",
    "    \"1587117610000\",\n",
    "    \"1586913303000\",\n",
    "    \"1586610083000\",\n",
    "    \"1586361914000\",\n",
    "    \"1585954821000\",\n",
    "    \"1585735387000\",\n",
    "    \"1585528208000\",\n",
    "    \"1585229419000\",\n",
    "    \"1584874812000\",\n",
    "    \"1584581236000\",\n",
    "    \"1581608419000\",\n",
    "    \"1589540408000\",\n",
    "    \"1589274011000\",\n",
    "    \"1588939218000\",\n",
    "    \"1588673798000\",\n",
    "    \"1588345217000\",\n",
    "    \"1588063310000\",\n",
    "    \"1587770106000\",\n",
    "    \"1587558821000\",\n",
    "    \"1587295804000\",\n",
    "    \"1587036700000\",\n",
    "    \"1586842439000\",\n",
    "    \"1586527218000\",\n",
    "    \"1586317395000\",\n",
    "    \"1585896342000\",\n",
    "    \"1585400413000\",\n",
    "    \"1585132217000\",\n",
    "    \"1584768613000\",\n",
    "    \"1584493217000\",\n",
    "    \"1579925153000\",  # back to Mar 19, the earliest\n",
    "]\n",
    "timestamp_list_COVCHINA.sort()\n",
    "timestamp_list_COVCHINA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestamp in timestamp_list_COVCHINA:\n",
    "    url = (\n",
    "        \"https://apigw.scmp.com/content-delivery/v1?operationName=gettopicbyentityuuid&variables=%7B%22latestContentsLimit%22%3A30%2C%22latestOpinionsLimit%22%3A30%2C%22entityUuid%22%3A%22c7985da3-2f6d-4540-a1c2-875c2d7881b6%22%2C%22articleTypeId%22%3A%22012d7708-2959-4b2b-9031-23e3d025a08d%22%2C%22applicationIds%22%3A%5B%222695b2c9-96ef-4fe4-96f8-ba20d0a020b3%22%5D%2C%22after%22%3A%22\"\n",
    "        + timestamp\n",
    "        + \"%22%7D&extensions=%7B%22persistedQuery%22%3A%7B%22version%22%3A1%2C%22sha256Hash%22%3A%22a78c49ca1280c93d31533f172e1a837d7e8aebf1215195364cb7fd85c76d2e0e%22%7D%7D\"\n",
    "    )\n",
    "    headers = {\"apikey\": \"MyYvyg8M9RTaevVlcIRhN5yRIqqVssNY\"}\n",
    "    r = requests.get(url=url, headers=headers)\n",
    "    j = r.json()[\"data\"][\"topic\"][\"latestContentsWithCursor\"][\"items\"]\n",
    "    for item in j:\n",
    "        url_list.append(item[\"urlAlias\"])\n",
    "        # print(item[\"urlAlias\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append new urls and save them in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exist.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(\"news_url.csv\"):\n",
    "    # creat the news url list for the first time\n",
    "    print(\"New file created.\")\n",
    "    news_url_list = []\n",
    "    for item in url_list:\n",
    "        news_url_list.append(\"https://www.scmp.com\" + item)\n",
    "    news_url_list = list(dict.fromkeys(news_url_list))  # rid duplicates\n",
    "    news_url_df = pd.DataFrame(data={\"news_url\": news_url_list})\n",
    "    news_url_df.to_csv(\n",
    "        \"../data/news_url.csv\", encoding=\"utf-8-sig\", header=\"column_names\", index=False\n",
    "    )\n",
    "else:\n",
    "    # there is already a news url list file\n",
    "    print(\"File already exist.\")\n",
    "    news_url_df = pd.read_csv(\"news_url.csv\")\n",
    "    news_url_list = list(news_url_df[\"news_url\"])\n",
    "    for item in url_list:\n",
    "        news_url_list.append(\"https://www.scmp.com\" + item)\n",
    "    news_url_list = list(dict.fromkeys(news_url_list))  # rid duplicates\n",
    "    news_url_df = pd.DataFrame(data={\"news_url\": news_url_list})\n",
    "    news_url_df.to_csv(\n",
    "        \"../data/news_url.csv\",\n",
    "        encoding=\"utf-8-sig\",\n",
    "        mode=\"w\",\n",
    "        header=\"column_names\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have collected 1177 news URLs in all.\n"
     ]
    }
   ],
   "source": [
    "# read news url list from csv\n",
    "news_url_df = pd.read_csv(\"../data/news_url.csv\")\n",
    "news_url_list = list(news_url_df[\"news_url\"].unique())\n",
    "print(\"We have collected %d news URLs in all.\" % len(list(news_url_df[\"news_url\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 386 loops, the connection breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing news at\n",
      " https://www.scmp.com/news/hong-kong/politics/article/3084984/coronavirus-labour-minister-expects-more-90-cent-hong-kong\n",
      "Parsing comments at\n",
      " https://www.scmp.com/scmp_comments/popup/3084984\n",
      "Loop 1 finished.\n",
      "Parsing news at\n",
      " https://www.scmp.com/business/companies/article/3084969/international-air-travel-starting-creep-back-complete-patchy\n",
      "Parsing comments at\n",
      " https://www.scmp.com/scmp_comments/popup/3084969\n",
      "Loop 2 finished.\n",
      "Parsing news at\n",
      " https://www.scmp.com/comment/opinion/article/3084828/why-chinas-post-lockdown-recovery-wont-work-test-case-reopening\n",
      "Parsing comments at\n",
      " https://www.scmp.com/scmp_comments/popup/3084828\n",
      "Loop 3 finished.\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "# news_url_list is the list of all news URLs\n",
    "for url in news_url_list[1174:]:\n",
    "    re_try = re.search(pattern=\"\\d{7}\", string=url)\n",
    "    if re_try != None:\n",
    "        news_id = re_try.group(0)\n",
    "        news_df = news_parser(url)  # parse news article\n",
    "        comments_id = news_id\n",
    "        comments_url = \"https://www.scmp.com/scmp_comments/popup/\" + comments_id\n",
    "        comments_df = comments_parser(comments_url)  # parse comments\n",
    "        if not news_df[\"article\"][0] == [\"\"]:\n",
    "            news_df.to_csv(\n",
    "                \"../data/news_new.csv\",\n",
    "                encoding=\"utf-8-sig\",\n",
    "                header=False,\n",
    "                index=False,\n",
    "                mode=\"a\",\n",
    "            )\n",
    "        if not comments_df[\"user_name\"][0] == None:\n",
    "            comments_df.to_csv(\n",
    "                \"../data/comments_new.csv\",\n",
    "                encoding=\"utf-8-sig\",\n",
    "                header=False,\n",
    "                index=False,\n",
    "                mode=\"a\",\n",
    "            )\n",
    "        counter += 1\n",
    "        print(\"Loop %d finished.\" % counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('learn': virtualenv)",
   "language": "python",
   "name": "python37464bitlearnvirtualenv1f4bee45323f459c9991f8fd39566062"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
